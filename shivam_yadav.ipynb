{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTqDemahQ3-y",
        "outputId": "064b467a-c953-414d-cf18-beceb08111b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the model performance for training set\n",
            "RMSE is 4.710901797319796\n",
            "\n",
            "\n",
            "The model performance for testing set\n",
            "RMSE is 4.687543527902972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "#this model is design to predict prices of houses in boston data set,also error in actual and calculated values by using Ml model\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn                                                       # contains ML algorithms and other stuff\n",
        "from sklearn.datasets import load_boston\n",
        "df=load_boston()\n",
        "df.keys()                                                            #to get all key in dataset\n",
        "\n",
        "#print(df.DESCR)                                                     to get description of dataset\n",
        "\n",
        "#print(df.feature_names)to get all colum name\n",
        "#print(df.target)to get prices of all houses\n",
        "#df.data contains values of colunm\n",
        "\n",
        "boston=pd.DataFrame(df.data,columns=df.feature_names)                 #converting dataset to tabular form\n",
        "\n",
        "#boston.head()                                                        to print tabular data of 5(its default value) dataset from dataframe\n",
        "\n",
        "boston['MEDV']=df.target                                              #adding new column name MEDV in data frame which will have values from target data set\n",
        "\n",
        "\n",
        "#boston.isnull()                                                      used to check for presence of any null value\n",
        "\n",
        "#boston.isnull().sum()                                                similar function as previous one but in smart way\n",
        "\n",
        "from sklearn.model_selection import train_test_split                 \n",
        "\n",
        "x=boston.drop('MEDV',axis=1)                                          # created another copy of data frame and deleted MEDV column from this\n",
        "y=boston['MEDV']                                                      #dataset which only contains target values\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.15,random_state=5)    #further division of x and y dataset into training and testing dataset\n",
        " \n",
        "#print(x_train.shape)                                                 thes 4 statements  we can use in order to get info about shapes of our datasets\n",
        "#print(x_test.shape)\n",
        "#print(y_train.shape)\n",
        "#print(y_test.shape)\n",
        " \n",
        "from sklearn.linear_model import LinearRegression                      #finally we are importing our ML model specially LinearRegression method   \n",
        "from sklearn.metrics import mean_squared_error                         #to find eror between predicted(calculated) and unseen values(actual)\n",
        "\n",
        "lin_model=LinearRegression()                                           #created model\n",
        "lin_model.fit(x_train,y_train)                                         #inserted training data\n",
        "\n",
        "y_train_predict=lin_model.predict(x_train)                            \n",
        "rmse=(np.sqrt(mean_squared_error(y_train,y_train_predict)))            #  to find mean squared error\n",
        "\n",
        "print(\"the model performance for training set\")\n",
        "print(\"RMSE is {}\".format(rmse))\n",
        "print(\"\\n\")\n",
        "\n",
        "#same thing on testing dataset\n",
        "\n",
        "y_test_predict=lin_model.predict(x_test)\n",
        "rmse=(np.sqrt(mean_squared_error(y_test,y_test_predict)))\n",
        "print(\"The model performance for testing set\")\n",
        "print(\"RMSE is {}\".format(rmse))\n",
        "\n",
        "\n"
      ]
    }
  ]
}